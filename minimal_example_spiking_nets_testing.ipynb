{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook provides a minimal example for using LFP to train a simple MLP-Spiking Neural Network (SNN) on MNIST.\n",
    "\n",
    "For more complex examples, refer to the experiment notebooks in ./nbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import snntorch as snn\n",
    "    from snntorch import utils as snnutils\n",
    "except ImportError:\n",
    "    print(\n",
    "        \"The SNN functionality of this package requires extra dependencies \",\n",
    "        \"which can be installed via pip install lfprop[snn] (or lfprop[full] for all dependencies).\",\n",
    "    )\n",
    "    raise ImportError(\"snntorch required; reinstall lfprop with option `snn` (pip install lfprop[snn])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lweber/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torcheval.metrics\n",
    "import torchvision.datasets as tvisiondata\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "\n",
    "import snntorch.functional as SF\n",
    "\n",
    "from experiment_utils.data.datasets import get_dataset\n",
    "from lfprop.model import activations\n",
    "from experiment_utils.data.dataloaders import get_dataloader\n",
    "from experiment_utils.data.transforms import get_transforms\n",
    "\n",
    "from lfprop.rewards import reward_functions as rewards  # Reward Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = \"./minimal-example-data\"\n",
    "os.makedirs(savepath, exist_ok=True)\n",
    "\n",
    "dataset_name = \"cifar10\"\n",
    "data_path = f\"/media/lweber/f3ed2aae-a7bf-4a55-b50d-ea8fb534f1f51/Datasets/{dataset_name}\"\n",
    "training_mode = \"lfp\"  # options: \"lfp\", \"surr\", \"both\"\n",
    "\n",
    "batch_size = 32  # 128\n",
    "n_channels = 1 if dataset_name == \"mnist\" else 3 #784\n",
    "n_outputs = 10\n",
    "n_steps = 15\n",
    "lr = 0.02\n",
    "momentum = 0.9\n",
    "epochs = 20\n",
    "model_name = \"lifresnetlike\" #\"lifresnetlike\" #deepersnn\n",
    "\n",
    "#lif_kwargs = {\"beta\": 0.9, \"reset_mechanism\": \"subtract\", \"surrogate_disable\": True}\n",
    "lif_kwargs = {\"beta\": 0.9, \"reset_mechanism\": \"subtract\", \"surrogate_disable\": False, \"spike_grad\": \"step\"}\n",
    "#lif_kwargs = {\"beta\": 0.9, \"reset_mechanism\": \"subtract\", \"surrogate_disable\": False, \"spike_grad\": \"atan\"}\n",
    "#surrogate_kwargs = {\"beta\": 0.9, \"reset_mechanism\": \"subtract\", \"surrogate_disable\": False, \"spike_grad\": \"step\"}\n",
    "surrogate_kwargs = {\"beta\": 0.9, \"reset_mechanism\": \"subtract\", \"surrogate_disable\": False, \"spike_grad\": \"atan\"}\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "None is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/None/resolve/main/preprocessor_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRepositoryNotFoundError\u001b[39m                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/transformers/utils/hub.py:424\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    423\u001b[39m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    426\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    427\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    428\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    429\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    430\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    431\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    435\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    436\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:862\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    861\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m862\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    863\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m    864\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    865\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m    866\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    867\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    868\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m    871\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m    877\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:969\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m    968\u001b[39m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m969\u001b[39m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1486\u001b[39m, in \u001b[36m_raise_on_head_call_error\u001b[39m\u001b[34m(head_call_error, force_download, local_files_only)\u001b[39m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1482\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error.response.status_code == \u001b[32m401\u001b[39m\n\u001b[32m   1483\u001b[39m ):\n\u001b[32m   1484\u001b[39m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[32m   1485\u001b[39m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1486\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[32m   1487\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1488\u001b[39m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1376\u001b[39m, in \u001b[36m_get_metadata_or_catch_error\u001b[39m\u001b[34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[39m\n\u001b[32m   1375\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1376\u001b[39m     metadata = \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:1296\u001b[39m, in \u001b[36mget_hf_file_metadata\u001b[39m\u001b[34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[39m\n\u001b[32m   1295\u001b[39m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1296\u001b[39m r = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1297\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mHEAD\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1299\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1301\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1303\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1304\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m hf_raise_for_status(r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:280\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     response = \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[32m    288\u001b[39m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/huggingface_hub/file_download.py:304\u001b[39m, in \u001b[36m_request_wrapper\u001b[39m\u001b[34m(method, url, follow_relative_redirects, **params)\u001b[39m\n\u001b[32m    303\u001b[39m response = get_session().request(method=method, url=url, **params)\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/huggingface_hub/utils/_http.py:458\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    450\u001b[39m     message = (\n\u001b[32m    451\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    452\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    456\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m make sure you are authenticated.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    457\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m response.status_code == \u001b[32m400\u001b[39m:\n",
      "\u001b[31mRepositoryNotFoundError\u001b[39m: 401 Client Error. (Request ID: Root=1-683f0319-7d2f8181259dbf2168ad10fb;41c94a30-c7ff-4c0b-9c0f-cbb81e1231a8)\n\nRepository Not Found for url: https://huggingface.co/None/resolve/main/preprocessor_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m training_data, _, _, _ = get_dataset(\n\u001b[32m      2\u001b[39m     dataset_name=dataset_name,\n\u001b[32m      3\u001b[39m     root_path=data_path,\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     transform=\u001b[43mget_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[32m      5\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m )\n\u001b[32m      7\u001b[39m validation_data, _, _, _ = get_dataset(\n\u001b[32m      8\u001b[39m     dataset_name=dataset_name,\n\u001b[32m      9\u001b[39m     root_path=data_path,\n\u001b[32m     10\u001b[39m     transform=get_transforms(dataset_name, \u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     11\u001b[39m     mode=\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m training_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work-code/layer-wise-feedback-propagation-github/experiment_utils/data/transforms.py:391\u001b[39m, in \u001b[36mget_transforms\u001b[39m\u001b[34m(dataset_name, mode, model_path)\u001b[39m\n\u001b[32m    389\u001b[39m     transforms = TRANSFORM_MAP[dataset_name][mode]\n\u001b[32m    390\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m     transforms = \u001b[43mHUGGINGFACE_TRANSFORMS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[38;5;66;03m# Return transforms\u001b[39;00m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m transforms\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work-code/layer-wise-feedback-propagation-github/experiment_utils/data/transforms.py:347\u001b[39m, in \u001b[36mget_vit_transform_beans\u001b[39m\u001b[34m(model_path)\u001b[39m\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_vit_transform_beans\u001b[39m(model_path):\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m     feature_extractor = \u001b[43mtransformers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mViTImageProcessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvit_transform_beans\u001b[39m(batch):\n\u001b[32m    350\u001b[39m         \u001b[38;5;66;03m# Take a list of PIL images and turn them to pixel values\u001b[39;00m\n\u001b[32m    351\u001b[39m         inputs = feature_extractor([x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m]], return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/transformers/image_processing_base.py:209\u001b[39m, in \u001b[36mImageProcessingMixin.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, **kwargs)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    207\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtoken\u001b[39m\u001b[33m\"\u001b[39m] = token\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m image_processor_dict, kwargs = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_image_processor_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.from_dict(image_processor_dict, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/transformers/image_processing_base.py:341\u001b[39m, in \u001b[36mImageProcessingMixin.get_image_processor_dict\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    338\u001b[39m image_processor_file = image_processor_filename\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    340\u001b[39m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m341\u001b[39m     resolved_image_processor_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_processor_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[32m    355\u001b[39m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[32m    356\u001b[39m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/transformers/utils/hub.py:266\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    209\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    210\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    211\u001b[39m     **kwargs,\n\u001b[32m    212\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    213\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    215\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    264\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    268\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/transformers/utils/hub.py:456\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[32m    455\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    457\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a local folder and is not a valid model identifier \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    458\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlisted on \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/models\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    459\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    460\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`token=<your_token>`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    461\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    462\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RevisionNotFoundError):\n\u001b[32m    463\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    464\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    465\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfor this model name. Check the model page at \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    466\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m for available revisions.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    467\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: None is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "training_data, _, _, _ = get_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    root_path=data_path,\n",
    "    transform=get_transforms(dataset_name, \"train\"),\n",
    "    mode=\"train\"\n",
    ")\n",
    "validation_data, _, _, _ = get_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    root_path=data_path,\n",
    "    transform=get_transforms(dataset_name, \"test\"),\n",
    "    mode=\"test\"\n",
    ")\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiment_utils.model.models import get_model\n",
    "\n",
    "# Create the first model and initialize weights\n",
    "model = get_model(model_name=model_name, n_channels=n_channels, n_outputs=n_outputs, device=device, **lif_kwargs)\n",
    "model.reset()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Copy the weights to use for the surrogate model\n",
    "model_surr = get_model(model_name=model_name, n_channels=n_channels, n_outputs=n_outputs, device=device, **surrogate_kwargs)\n",
    "model_surr.load_state_dict(model.state_dict())\n",
    "model_surr.reset()\n",
    "model_surr.to(device)\n",
    "model_surr.eval()\n",
    "\n",
    "def name_modules(module, name):\n",
    "    \"\"\"\n",
    "    Recursive function to name modules for debugging \n",
    "    \"\"\"\n",
    "    for cname, child in module.named_children():\n",
    "        child.tmpname = cname if name == \"root\" else f\"{name}.{cname}\"\n",
    "        name_modules(child, child.tmpname)\n",
    "\n",
    "name_modules(model, \"root\")\n",
    "name_modules(model_surr, \"root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up LFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SNN-Propagator\n",
    "from lfprop.propagation.propagator_snn import LFPSNNEpsilonComposite\n",
    "\n",
    "propagation_composite = LFPSNNEpsilonComposite(epsilon=1e-6)\n",
    "\n",
    "# Initialize the Reward Function.\n",
    "reward_func = rewards.SnnCorrectClassRewardSpikesRateCoded(device)\n",
    "\n",
    "# LFP writes its updates into the .grad attribute of the model parameters, and can thus utilize standard torch optimizers\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_surr = torch.optim.SGD(model_surr.parameters(), lr=lr, momentum=momentum)\n",
    "loss_fn_surr = SF.loss.ce_rate_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Simple Evaluation using torcheval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(loader, n_steps: int = 15):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a single dataset\n",
    "    \"\"\"\n",
    "    eval_metrics = {\n",
    "        \"reward\": torcheval.metrics.Mean(device=device),\n",
    "        \"accuracy\": torcheval.metrics.MulticlassAccuracy(average=\"micro\", num_classes=10, k=1, device=device),\n",
    "    }\n",
    "\n",
    "    model.eval()\n",
    "    model.reset()\n",
    "\n",
    "    # Iterate over Data Loader\n",
    "    for index, (inputs, labels) in tqdm(enumerate(loader), desc=\"Evaluating\", total=len(loader)):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = (labels).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get model predictions\n",
    "            u_rec, spk_rec = [], []\n",
    "            for step in tqdm(range(n_steps), disable=True):  # [ ] move this into the fwd method of the model?\n",
    "                y = model(inputs)\n",
    "                spk_out, u_out = y\n",
    "                u_rec.append(u_out)\n",
    "                spk_rec.append(spk_out)\n",
    "\n",
    "            spikes = torch.stack(spk_rec, dim=0)\n",
    "            membrane_potential = torch.stack(u_rec, dim=0)\n",
    "\n",
    "            # Get rewards\n",
    "            reward = reward_func(spikes=spikes, potentials=membrane_potential, labels=labels)\n",
    "            outputs = reward_func.get_predictions(spikes=spikes, potentials=membrane_potential)\n",
    "\n",
    "        for k, v in eval_metrics.items():\n",
    "            if k == \"reward\":\n",
    "                eval_metrics[k].update(reward)\n",
    "            else:\n",
    "                eval_metrics[k].update(outputs, labels)\n",
    "\n",
    "    return_dict = {m: metric.compute().detach().cpu().numpy() for m, metric in eval_metrics.items()}\n",
    "    model.reset()\n",
    "    # Return evaluation\n",
    "    return return_dict\n",
    "\n",
    "def eval_model_surr(loader, n_steps=15):\n",
    "    eval_metrics = {\n",
    "        \"loss\": torcheval.metrics.Mean(device=device),\n",
    "        \"accuracy\": torcheval.metrics.MulticlassAccuracy(average=\"micro\", num_classes=n_outputs, k=1, device=device),\n",
    "    }\n",
    "    model_surr.eval()\n",
    "    model_surr.reset()\n",
    "    for index, (inputs, labels) in tqdm(enumerate(loader), desc=\"Evaluating (Grad)\", total=len(loader)):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            spk_rec = []\n",
    "            for step in range(n_steps):\n",
    "                spk_out, _ = model_surr(inputs)\n",
    "                spk_rec.append(spk_out)\n",
    "            spikes = torch.stack(spk_rec, dim=0)\n",
    "            loss = loss_fn_surr(spk_out=spikes, targets=labels)\n",
    "            outputs = spikes.sum(0).argmax(-1)\n",
    "        eval_metrics[\"loss\"].update(loss)\n",
    "        eval_metrics[\"accuracy\"].update(outputs, labels)\n",
    "    return {m: metric.compute().detach().cpu().numpy() for m, metric in eval_metrics.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (block1): LFPEpsilonSNN(\n",
      "    (module): SpikingLayer(\n",
      "      (parameterized_layer): NoisyWrapper(\n",
      "        (module): Conv2d(3, 30, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      )\n",
      "      (spike_mechanism): Leaky(\n",
      "        (spike_grad): Step()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block2): LFPEpsilonSNN(\n",
      "    (module): SpikingLayer(\n",
      "      (parameterized_layer): NoisyWrapper(\n",
      "        (module): Conv2d(30, 150, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (spike_mechanism): Leaky(\n",
      "        (spike_grad): Step()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block3): LFPEpsilonSNN(\n",
      "    (module): SpikingLayer(\n",
      "      (parameterized_layer): NoisyWrapper(\n",
      "        (module): Conv2d(150, 250, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      )\n",
      "      (spike_mechanism): Leaky(\n",
      "        (spike_grad): Step()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pool3): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (block4): LFPEpsilonSNN(\n",
      "    (module): SpikingLayer(\n",
      "      (parameterized_layer): NoisyWrapper(\n",
      "        (module): Conv2d(250, 200, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      )\n",
      "      (spike_mechanism): Leaky(\n",
      "        (spike_grad): Step()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (skip_connection): LFPEpsilonSNN(\n",
      "    (module): SpikingLayer(\n",
      "      (parameterized_layer): Sequential(\n",
      "        (0): NoisyWrapper(\n",
      "          (module): Conv2d(150, 250, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        )\n",
      "        (1): Interpolate()\n",
      "      )\n",
      "      (spike_mechanism): Leaky(\n",
      "        (spike_grad): Step()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc): LFPEpsilonSNN(\n",
      "    (module): SpikingLayer(\n",
      "      (parameterized_layer): NoisyWrapper(\n",
      "        (module): Linear(in_features=1800, out_features=10, bias=False)\n",
      "      )\n",
      "      (spike_mechanism): Leaky(\n",
      "        (spike_grad): Step()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lweber/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/lxt/core.py:362: UserWarning: This functionality is not yet fully tested. Please check the model after removing the composite.\n",
      "  warn(\n",
      " 29%|██▊       | 112/391 [01:06<02:45,  1.69it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 101\u001b[39m\n\u001b[32m     98\u001b[39m labels = labels.to(device)\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# LFP step\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m lfp_fwd, lfp_bwd = \u001b[43mlfp_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m==\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m==\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m lfp_fwd_total += lfp_fwd\n\u001b[32m    103\u001b[39m lfp_bwd_total += lfp_bwd\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 64\u001b[39m, in \u001b[36mlfp_step\u001b[39m\u001b[34m(inputs, labels, n_steps, print_model)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[32m     63\u001b[39m t2 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspikes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m t3 = time.time()\n\u001b[32m     66\u001b[39m backward_time = t3 - t2\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/torch/autograd/__init__.py:496\u001b[39m, in \u001b[36mgrad\u001b[39m\u001b[34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[39m\n\u001b[32m    492\u001b[39m     result = _vmap_internals._vmap(vjp, \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, allow_none_pass_through=\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[32m    493\u001b[39m         grad_outputs_\n\u001b[32m    494\u001b[39m     )\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     result = \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[32m    506\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[32m    507\u001b[39m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[32m    508\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[32m    509\u001b[39m     ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def grad_step(inputs, labels, n_steps=15):\n",
    "    model_surr.train()\n",
    "    model_surr.reset()\n",
    "    optimizer_surr.zero_grad()\n",
    "\n",
    "    # Forward pass through time\n",
    "    spk_rec = []\n",
    "    t0 = time.time()\n",
    "    for step in range(n_steps):\n",
    "        spk_out, _ = model_surr(inputs)\n",
    "        spk_rec.append(spk_out)\n",
    "    spikes = torch.stack(spk_rec, dim=0)\n",
    "    t1 = time.time()\n",
    "    forward_time = t1 - t0\n",
    "\n",
    "    # Compute loss and backward\n",
    "    t2 = time.time()\n",
    "    loss = loss_fn_surr(spk_out=spikes, targets=labels)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model_surr.parameters(), 3.0, 2.0)\n",
    "    optimizer_surr.step()\n",
    "    t3 = time.time()\n",
    "    backward_time = t3 - t2\n",
    "\n",
    "    model_surr.reset()\n",
    "    model_surr.eval()\n",
    "    return loss.item(), forward_time, backward_time\n",
    "\n",
    "def lfp_step(inputs, labels, n_steps: int = 15, print_model: bool = False):\n",
    "    \"\"\"\n",
    "    Performs a single training step using LFP. This is quite similar to a standard gradient descent training loop.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    model.reset()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    with propagation_composite.context(model) as modified:\n",
    "        inputs = inputs.detach().requires_grad_(True)\n",
    "        \n",
    "        if print_model:\n",
    "            print(modified)\n",
    "        \n",
    "        # Forward pass\n",
    "        u_rec, spk_rec = [], []\n",
    "        t0 = time.time()\n",
    "        for step in range(n_steps):\n",
    "            outputs = modified(inputs)\n",
    "            spk_out, u_out = outputs\n",
    "            u_rec.append(u_out)\n",
    "            spk_rec.append(spk_out)\n",
    "        spikes = torch.stack(spk_rec, dim=0)\n",
    "        membrane_potential = torch.stack(u_rec, dim=0)\n",
    "        t1 = time.time()\n",
    "        forward_time = t1 - t0\n",
    "\n",
    "        # Reward\n",
    "        reward = torch.from_numpy(reward_func(spikes=spikes, labels=labels).detach().cpu().numpy()).to(device)\n",
    "        reward /= n_steps\n",
    "\n",
    "        # Backward pass\n",
    "        t2 = time.time()\n",
    "        torch.autograd.grad((spikes,), (inputs,), grad_outputs=(reward,), retain_graph=False)\n",
    "        t3 = time.time()\n",
    "        backward_time = t3 - t2\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if not hasattr(param, 'feedback'):\n",
    "            print(f\"Parameter {name} does not have feedback attribute.\")\n",
    "        param.grad = -param.feedback\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0, 2.0)\n",
    "\n",
    "    t4 = time.time()\n",
    "    optimizer.step()\n",
    "    t5 = time.time()\n",
    "\n",
    "    backward_time = t3 - t2 + t5-t4\n",
    "    model.reset()\n",
    "    model.eval()\n",
    "    return forward_time, backward_time\n",
    "\n",
    "# Initialize dictionary to store timing info per epoch\n",
    "timing_stats = {}\n",
    "# Initialize dictionary to store evaluation stats per epoch\n",
    "eval_stats_per_epoch = {}\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    lfp_fwd_total, lfp_bwd_total = 0.0, 0.0\n",
    "    surr_fwd_total, surr_bwd_total = 0.0, 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with tqdm(total=len(training_loader)) as pbar:\n",
    "        for index, (inputs, labels) in enumerate(training_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # LFP step\n",
    "            lfp_fwd, lfp_bwd = lfp_step(inputs, labels, n_steps=n_steps, print_model=index==0 and epoch==0)\n",
    "            lfp_fwd_total += lfp_fwd\n",
    "            lfp_bwd_total += lfp_bwd\n",
    "\n",
    "            # Surrogate gradient step\n",
    "            surr_loss, surr_fwd, surr_bwd = grad_step(inputs, labels, n_steps=n_steps)\n",
    "            surr_fwd_total += surr_fwd\n",
    "            surr_bwd_total += surr_bwd\n",
    "\n",
    "            num_batches += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Store timing info for this epoch\n",
    "    timing_stats[epoch] = {\n",
    "        \"lfp_forward\": lfp_fwd_total,\n",
    "        \"lfp_backward\": lfp_bwd_total,\n",
    "        \"lfp_total\": lfp_fwd_total + lfp_bwd_total,\n",
    "        \"surr_forward\": surr_fwd_total,\n",
    "        \"surr_backward\": surr_bwd_total,\n",
    "        \"surr_total\": surr_fwd_total + surr_bwd_total,\n",
    "        \"num_batches\": num_batches\n",
    "    }\n",
    "\n",
    "    # Evaluate both models\n",
    "    eval_stats_train_lfp = eval_model(training_loader, n_steps=n_steps)\n",
    "    eval_stats_val_lfp = eval_model(validation_loader, n_steps=n_steps)\n",
    "    eval_stats_train_surr = eval_model_surr(training_loader, n_steps=n_steps)\n",
    "    eval_stats_val_surr = eval_model_surr(validation_loader, n_steps=n_steps)\n",
    "\n",
    "    # Store evaluation stats for this epoch\n",
    "    eval_stats_per_epoch[epoch] = {\n",
    "        \"eval_train_lfp\": eval_stats_train_lfp,\n",
    "        \"eval_val_lfp\": eval_stats_val_lfp,\n",
    "        \"eval_train_surr\": eval_stats_train_surr,\n",
    "        \"eval_val_surr\": eval_stats_val_surr\n",
    "    }\n",
    "\n",
    "    # Also store in timing_stats for compatibility\n",
    "    timing_stats[epoch][\"eval_train_lfp\"] = eval_stats_train_lfp\n",
    "    timing_stats[epoch][\"eval_val_lfp\"] = eval_stats_val_lfp\n",
    "    timing_stats[epoch][\"eval_train_surr\"] = eval_stats_train_surr\n",
    "    timing_stats[epoch][\"eval_val_surr\"] = eval_stats_val_surr\n",
    "\n",
    "    # Print aggregated timing info for this epoch\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs} Timing:\\n\"\n",
    "        f\"  LFP:   forward {lfp_fwd_total:.4f}s, backward {lfp_bwd_total:.4f}s\\n\"\n",
    "        f\"  Grad:   forward {surr_fwd_total:.4f}s, backward {surr_bwd_total:.4f}s\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs}:\\n\"\n",
    "        f\"  LFP:   (Train Reward) {eval_stats_train_lfp['reward']:.2f}; (Train Acc) {eval_stats_train_lfp['accuracy']:.2f}; \"\n",
    "        f\"(Val Reward) {eval_stats_val_lfp['reward']:.2f}; (Val Acc) {eval_stats_val_lfp['accuracy']:.2f}\\n\"\n",
    "        f\"  Grad:   (Train Loss) {eval_stats_train_surr['loss']:.2f}; (Train Acc) {eval_stats_train_surr['accuracy']:.2f}; \"\n",
    "        f\"(Val Loss) {eval_stats_val_surr['loss']:.2f}; (Val Acc) {eval_stats_val_surr['accuracy']:.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# Set font properties and plot style (copied from beans-vit-training)\n",
    "font_path = plt.matplotlib.get_data_path() + \"/fonts/ttf/cmr10.ttf\"\n",
    "cmfont = font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.serif\"] = cmfont.get_name()\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"cm\"\n",
    "plt.rcParams[\"font.size\"] = 15\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "plt.rcParams[\"axes.formatter.use_mathtext\"] = True\n",
    "plt.rcParams['axes.linewidth'] = 1.5\n",
    "\n",
    "# Prepare data\n",
    "epochs_range = list(eval_stats_per_epoch.keys())\n",
    "acc_lfp = [eval_stats_per_epoch[e][\"eval_val_lfp\"][\"accuracy\"] for e in epochs_range]\n",
    "acc_surr = [eval_stats_per_epoch[e][\"eval_val_surr\"][\"accuracy\"] for e in epochs_range]\n",
    "runtime_lfp = [timing_stats[e][\"lfp_total\"] for e in epochs_range]\n",
    "runtime_surr = [timing_stats[e][\"surr_total\"] for e in epochs_range]\n",
    "\n",
    "# Colormap and labels\n",
    "colors = np.linspace(0, 1, 2)\n",
    "palette = cm.get_cmap(\"Set1\")(colors)\n",
    "pastel = 0.3\n",
    "palette = (1 - pastel) * palette + pastel * np.ones((2, 4))\n",
    "\n",
    "LABELS = {\n",
    "    \"lfp\": r\"LFP-$\\varepsilon$\",\n",
    "    \"surr\": r\"Surrogate Grad\",\n",
    "}\n",
    "\n",
    "# Plot accuracy over epochs\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "xaxis = np.arange(len(epochs_range))\n",
    "\n",
    "ax.plot(xaxis, acc_lfp, color=palette[0], label=LABELS[\"lfp\"], linewidth=3.5, alpha=1)\n",
    "ax.plot(xaxis, acc_surr, color=palette[1], label=LABELS[\"surr\"], linewidth=3.5, alpha=1)\n",
    "\n",
    "linelocs = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "ax.hlines(\n",
    "    linelocs,\n",
    "    xmin=-1,\n",
    "    xmax=xaxis[-1],\n",
    "    color=(0.5, 0.5, 0.5, 1),\n",
    "    linewidth=1.5,\n",
    "    zorder=0,\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\"Validation Accuracy [%]\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylim([0.0, 1.01])\n",
    "ax.set_yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_xlim([0.0, xaxis[-1]])\n",
    "ax.set_yticklabels([0, 20, 40, 60, 80, 100])\n",
    "ax.tick_params(length=6, width=2)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot runtime vs. accuracy (logscale x-axis)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "ax.plot(np.cumsum(runtime_lfp), acc_lfp, color=palette[0], label=LABELS[\"lfp\"], marker=\"o\", linewidth=3.5)\n",
    "ax.plot(np.cumsum(runtime_surr), acc_surr, color=palette[1], label=LABELS[\"surr\"], marker=\"o\", linewidth=3.5)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Cumulative Runtime (s, log scale)\")\n",
    "ax.set_ylabel(\"Validation Accuracy [%]\")\n",
    "ax.set_ylim([0.0, 1.01])\n",
    "ax.set_yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels([0, 20, 40, 60, 80, 100])\n",
    "ax.tick_params(length=6, width=2)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfprop-KukTaqIE-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
