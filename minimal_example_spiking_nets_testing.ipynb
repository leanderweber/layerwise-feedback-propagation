{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook provides a minimal example for using LFP to train a simple MLP-Spiking Neural Network (SNN) on MNIST.\n",
    "\n",
    "For more complex examples, refer to the experiment notebooks in ./nbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import snntorch as snn\n",
    "    from snntorch import utils as snnutils\n",
    "except ImportError:\n",
    "    print(\n",
    "        \"The SNN functionality of this package requires extra dependencies \",\n",
    "        \"which can be installed via pip install lfprop[snn] (or lfprop[full] for all dependencies).\",\n",
    "    )\n",
    "    raise ImportError(\"snntorch required; reinstall lfprop with option `snn` (pip install lfprop[snn])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lweber/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torcheval.metrics\n",
    "import torchvision.datasets as tvisiondata\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "\n",
    "import snntorch.functional as SF\n",
    "\n",
    "from experiment_utils.data.datasets import get_dataset\n",
    "from lfprop.model import activations\n",
    "from experiment_utils.data.dataloaders import get_dataloader\n",
    "from experiment_utils.data.transforms import get_transforms\n",
    "\n",
    "from lfprop.rewards import reward_functions as rewards  # Reward Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "savepath = \"./minimal-example-data\"\n",
    "os.makedirs(savepath, exist_ok=True)\n",
    "\n",
    "dataset_name = \"mnist\"\n",
    "data_path = f\"/media/lweber/f3ed2aae-a7bf-4a55-b50d-ea8fb534f1f51/Datasets/{dataset_name}\"\n",
    "training_mode = \"lfp\"  # options: \"lfp\", \"surr\", \"both\"\n",
    "\n",
    "batch_size = 128  # 128\n",
    "n_channels = 1 if dataset_name == \"mnist\" else 3 #784\n",
    "n_outputs = 10\n",
    "n_steps = 15\n",
    "lr = 0.02\n",
    "momentum = 0.9\n",
    "epochs = 20\n",
    "model_name = \"lifcnn\"\n",
    "\n",
    "lif_kwargs = {\"beta\": 0.9, \"reset_mechanism\": \"subtract\", \"surrogate_disable\": True}\n",
    "#lif_kwargs = {\"beta\": 0.9, \"reset_mechanism\": \"subtract\", \"surrogate_disable\": False, \"spike_grad\": \"step\"}\n",
    "#lif_kwargs = {\"beta\": 0.9, \"reset_mechanism\": \"subtract\", \"surrogate_disable\": False, \"spike_grad\": \"atan\"}\n",
    "#surrogate_kwargs = {\"beta\": 0.9, \"reset_mechanism\": \"subtract\", \"surrogate_disable\": False, \"spike_grad\": \"step\"}\n",
    "surrogate_kwargs = {\"beta\": 0.9, \"reset_mechanism\": \"subtract\", \"surrogate_disable\": False, \"spike_grad\": \"atan\"}\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, _, _, _ = get_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    root_path=data_path,\n",
    "    transform=get_transforms(dataset_name, \"train\"),\n",
    "    mode=\"train\"\n",
    ")\n",
    "validation_data, _, _, _ = get_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    root_path=data_path,\n",
    "    transform=get_transforms(dataset_name, \"test\"),\n",
    "    mode=\"test\"\n",
    ")\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "from experiment_utils.model.models import get_model\n",
    "\n",
    "# Create the first model and initialize weights\n",
    "model = get_model(model_name=model_name, n_channels=n_channels, n_outputs=n_outputs, device=device, **lif_kwargs)\n",
    "model.reset()\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Copy the weights to use for the surrogate model\n",
    "model_surr = get_model(model_name=model_name, n_channels=n_channels, n_outputs=n_outputs, device=device, **surrogate_kwargs)\n",
    "model_surr.load_state_dict(model.state_dict())\n",
    "model_surr.reset()\n",
    "model_surr.to(device)\n",
    "model_surr.eval()\n",
    "\n",
    "def name_modules(module, name):\n",
    "    \"\"\"\n",
    "    Recursive function to name modules for debugging \n",
    "    \"\"\"\n",
    "    for cname, child in module.named_children():\n",
    "        child.tmpname = cname if name == \"root\" else f\"{name}.{cname}\"\n",
    "        name_modules(child, child.tmpname)\n",
    "\n",
    "name_modules(model, \"root\")\n",
    "name_modules(model_surr, \"root\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up LFP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SNN-Propagator\n",
    "from lfprop.propagation.propagator_snn import LFPSNNEpsilonComposite\n",
    "\n",
    "propagation_composite = LFPSNNEpsilonComposite(epsilon=1e-6)\n",
    "\n",
    "# Initialize the Reward Function.\n",
    "reward_func = rewards.SnnCorrectClassRewardSpikesRateCoded(device)\n",
    "\n",
    "# LFP writes its updates into the .grad attribute of the model parameters, and can thus utilize standard torch optimizers\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_surr = torch.optim.SGD(model_surr.parameters(), lr=lr, momentum=momentum)\n",
    "loss_fn_surr = SF.loss.ce_rate_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Simple Evaluation using torcheval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(loader, n_steps: int = 15):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a single dataset\n",
    "    \"\"\"\n",
    "    eval_metrics = {\n",
    "        \"reward\": torcheval.metrics.Mean(device=device),\n",
    "        \"accuracy\": torcheval.metrics.MulticlassAccuracy(average=\"micro\", num_classes=10, k=1, device=device),\n",
    "    }\n",
    "\n",
    "    model.eval()\n",
    "    model.reset()\n",
    "\n",
    "    # Iterate over Data Loader\n",
    "    for index, (inputs, labels) in tqdm(enumerate(loader), desc=\"Evaluating\", total=len(loader)):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = (labels).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get model predictions\n",
    "            u_rec, spk_rec = [], []\n",
    "            for step in tqdm(range(n_steps), disable=True):  # [ ] move this into the fwd method of the model?\n",
    "                y = model(inputs)\n",
    "                spk_out, u_out = y\n",
    "                u_rec.append(u_out)\n",
    "                spk_rec.append(spk_out)\n",
    "\n",
    "            spikes = torch.stack(spk_rec, dim=0)\n",
    "            membrane_potential = torch.stack(u_rec, dim=0)\n",
    "\n",
    "            # Get rewards\n",
    "            reward = reward_func(spikes=spikes, potentials=membrane_potential, labels=labels)\n",
    "            outputs = reward_func.get_predictions(spikes=spikes, potentials=membrane_potential)\n",
    "\n",
    "        for k, v in eval_metrics.items():\n",
    "            if k == \"reward\":\n",
    "                eval_metrics[k].update(reward)\n",
    "            else:\n",
    "                eval_metrics[k].update(outputs, labels)\n",
    "\n",
    "    return_dict = {m: metric.compute().detach().cpu().numpy() for m, metric in eval_metrics.items()}\n",
    "    model.reset()\n",
    "    # Return evaluation\n",
    "    return return_dict\n",
    "\n",
    "def eval_model_surr(loader, n_steps=15):\n",
    "    eval_metrics = {\n",
    "        \"loss\": torcheval.metrics.Mean(device=device),\n",
    "        \"accuracy\": torcheval.metrics.MulticlassAccuracy(average=\"micro\", num_classes=n_outputs, k=1, device=device),\n",
    "    }\n",
    "    model_surr.eval()\n",
    "    model_surr.reset()\n",
    "    for index, (inputs, labels) in tqdm(enumerate(loader), desc=\"Evaluating (Grad)\", total=len(loader)):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        with torch.no_grad():\n",
    "            spk_rec = []\n",
    "            for step in range(n_steps):\n",
    "                spk_out, _ = model_surr(inputs)\n",
    "                spk_rec.append(spk_out)\n",
    "            spikes = torch.stack(spk_rec, dim=0)\n",
    "            loss = loss_fn_surr(spk_out=spikes, targets=labels)\n",
    "            outputs = spikes.sum(0).argmax(-1)\n",
    "        eval_metrics[\"loss\"].update(loss)\n",
    "        eval_metrics[\"accuracy\"].update(outputs, labels)\n",
    "    return {m: metric.compute().detach().cpu().numpy() for m, metric in eval_metrics.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/469 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lweber/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/lxt/core.py:362: UserWarning: This functionality is not yet fully tested. Please check the model after removing the composite.\n",
      "  warn(\n",
      "  0%|          | 2/469 [00:00<02:58,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/469 [00:01<01:50,  4.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 6/469 [00:01<01:40,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 7/469 [00:01<01:34,  4.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 9/469 [00:02<01:23,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 11/469 [00:02<01:17,  5.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 13/469 [00:02<01:13,  6.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 15/469 [00:03<01:13,  6.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 17/469 [00:03<01:16,  5.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 19/469 [00:03<01:24,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 21/469 [00:04<01:22,  5.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 23/469 [00:04<01:20,  5.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 25/469 [00:04<01:18,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 27/469 [00:05<01:17,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 29/469 [00:05<01:18,  5.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 30/469 [00:05<01:25,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LifCNN(\n",
      "  (classifier): Sequential(\n",
      "    (0): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(1, 12, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (2): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Conv2d(12, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Flatten(start_dim=1, end_dim=-1)\n",
      "    (5): LFPEpsilonSNN(\n",
      "      (module): SpikingLayer(\n",
      "        (parameterized_layer): Linear(in_features=1024, out_features=10, bias=True)\n",
      "        (spike_mechanism): Leaky()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 104\u001b[39m\n\u001b[32m    101\u001b[39m lfp_bwd_total += lfp_bwd\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# Surrogate gradient step\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m surr_loss, surr_fwd, surr_bwd = \u001b[43mgrad_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m surr_fwd_total += surr_fwd\n\u001b[32m    106\u001b[39m surr_bwd_total += surr_bwd\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mgrad_step\u001b[39m\u001b[34m(inputs, labels, n_steps)\u001b[39m\n\u001b[32m     10\u001b[39m t0 = time.time()\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_steps):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     spk_out, _ = \u001b[43mmodel_surr\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     spk_rec.append(spk_out)\n\u001b[32m     14\u001b[39m spikes = torch.stack(spk_rec, dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work-code/layer-wise-feedback-propagation-github/lfprop/model/spiking_networks.py:228\u001b[39m, in \u001b[36mLifCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m    224\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    225\u001b[39m \u001b[33;03m    Forwards input through network\u001b[39;00m\n\u001b[32m    226\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    230\u001b[39m     \u001b[38;5;66;03m# Return output\u001b[39;00m\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1743\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1739\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n\u001b[32m   1741\u001b[39m \u001b[38;5;66;03m# torchrec tests the code consistency with the following code\u001b[39;00m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# fmt: off\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1744\u001b[39m     forward_call = (\u001b[38;5;28mself\u001b[39m._slow_forward \u001b[38;5;28;01mif\u001b[39;00m torch._C._get_tracing_state() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.forward)\n\u001b[32m   1745\u001b[39m     \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m     \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def grad_step(inputs, labels, n_steps=15):\n",
    "    model_surr.train()\n",
    "    model_surr.reset()\n",
    "    optimizer_surr.zero_grad()\n",
    "\n",
    "    # Forward pass through time\n",
    "    spk_rec = []\n",
    "    t0 = time.time()\n",
    "    for step in range(n_steps):\n",
    "        spk_out, _ = model_surr(inputs)\n",
    "        spk_rec.append(spk_out)\n",
    "    spikes = torch.stack(spk_rec, dim=0)\n",
    "    t1 = time.time()\n",
    "    forward_time = t1 - t0\n",
    "\n",
    "    # Compute loss and backward\n",
    "    t2 = time.time()\n",
    "    loss = loss_fn_surr(spk_out=spikes, targets=labels)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(model_surr.parameters(), 3.0, 2.0)\n",
    "    optimizer_surr.step()\n",
    "    t3 = time.time()\n",
    "    backward_time = t3 - t2\n",
    "\n",
    "    model_surr.reset()\n",
    "    model_surr.eval()\n",
    "    return loss.item(), forward_time, backward_time\n",
    "\n",
    "def lfp_step(inputs, labels, n_steps: int = 15):\n",
    "    \"\"\"\n",
    "    Performs a single training step using LFP. This is quite similar to a standard gradient descent training loop.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    model.reset()\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        optimizer.zero_grad()\n",
    "        with propagation_composite.context(model) as modified:\n",
    "            inputs = inputs.detach().requires_grad_(True)\n",
    "            \n",
    "            # Forward pass\n",
    "            u_rec, spk_rec = [], []\n",
    "            t0 = time.time()\n",
    "            for step in range(n_steps):\n",
    "                outputs = modified(inputs)\n",
    "                spk_out, u_out = outputs\n",
    "                u_rec.append(u_out)\n",
    "                spk_rec.append(spk_out)\n",
    "            spikes = torch.stack(spk_rec, dim=0)\n",
    "            membrane_potential = torch.stack(u_rec, dim=0)\n",
    "            t1 = time.time()\n",
    "            forward_time = t1 - t0\n",
    "\n",
    "            # Reward\n",
    "            reward = torch.from_numpy(reward_func(spikes=spikes, labels=labels).detach().cpu().numpy()).to(device)\n",
    "            reward /= n_steps\n",
    "\n",
    "            # Backward pass\n",
    "            t2 = time.time()\n",
    "            torch.autograd.grad((spikes,), (inputs,), grad_outputs=(reward,), retain_graph=False)\n",
    "            t3 = time.time()\n",
    "            backward_time = t3 - t2\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        param.grad = -param.feedback\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0, 2.0)\n",
    "\n",
    "    t4 = time.time()\n",
    "    optimizer.step()\n",
    "    t5 = time.time()\n",
    "\n",
    "    backward_time = t3 - t2 + t5-t4\n",
    "    model.reset()\n",
    "    model.eval()\n",
    "    return forward_time, backward_time\n",
    "\n",
    "# Initialize dictionary to store timing info per epoch\n",
    "timing_stats = {}\n",
    "# Initialize dictionary to store evaluation stats per epoch\n",
    "eval_stats_per_epoch = {}\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    lfp_fwd_total, lfp_bwd_total = 0.0, 0.0\n",
    "    surr_fwd_total, surr_bwd_total = 0.0, 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with tqdm(total=len(training_loader)) as pbar:\n",
    "        for index, (inputs, labels) in enumerate(training_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # LFP step\n",
    "            lfp_fwd, lfp_bwd = lfp_step(inputs, labels, n_steps=n_steps)\n",
    "            lfp_fwd_total += lfp_fwd\n",
    "            lfp_bwd_total += lfp_bwd\n",
    "\n",
    "            # Surrogate gradient step\n",
    "            surr_loss, surr_fwd, surr_bwd = grad_step(inputs, labels, n_steps=n_steps)\n",
    "            surr_fwd_total += surr_fwd\n",
    "            surr_bwd_total += surr_bwd\n",
    "\n",
    "            num_batches += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Store timing info for this epoch\n",
    "    timing_stats[epoch] = {\n",
    "        \"lfp_forward\": lfp_fwd_total,\n",
    "        \"lfp_backward\": lfp_bwd_total,\n",
    "        \"lfp_total\": lfp_fwd_total + lfp_bwd_total,\n",
    "        \"surr_forward\": surr_fwd_total,\n",
    "        \"surr_backward\": surr_bwd_total,\n",
    "        \"surr_total\": surr_fwd_total + surr_bwd_total,\n",
    "        \"num_batches\": num_batches\n",
    "    }\n",
    "\n",
    "    # Evaluate both models\n",
    "    eval_stats_train_lfp = eval_model(training_loader, n_steps=n_steps)\n",
    "    eval_stats_val_lfp = eval_model(validation_loader, n_steps=n_steps)\n",
    "    eval_stats_train_surr = eval_model_surr(training_loader, n_steps=n_steps)\n",
    "    eval_stats_val_surr = eval_model_surr(validation_loader, n_steps=n_steps)\n",
    "\n",
    "    # Store evaluation stats for this epoch\n",
    "    eval_stats_per_epoch[epoch] = {\n",
    "        \"eval_train_lfp\": eval_stats_train_lfp,\n",
    "        \"eval_val_lfp\": eval_stats_val_lfp,\n",
    "        \"eval_train_surr\": eval_stats_train_surr,\n",
    "        \"eval_val_surr\": eval_stats_val_surr\n",
    "    }\n",
    "\n",
    "    # Also store in timing_stats for compatibility\n",
    "    timing_stats[epoch][\"eval_train_lfp\"] = eval_stats_train_lfp\n",
    "    timing_stats[epoch][\"eval_val_lfp\"] = eval_stats_val_lfp\n",
    "    timing_stats[epoch][\"eval_train_surr\"] = eval_stats_train_surr\n",
    "    timing_stats[epoch][\"eval_val_surr\"] = eval_stats_val_surr\n",
    "\n",
    "    # Print aggregated timing info for this epoch\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs} Timing:\\n\"\n",
    "        f\"  LFP:   forward {lfp_fwd_total:.4f}s, backward {lfp_bwd_total:.4f}s\\n\"\n",
    "        f\"  Grad:   forward {surr_fwd_total:.4f}s, backward {surr_bwd_total:.4f}s\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Epoch {epoch+1}/{epochs}:\\n\"\n",
    "        f\"  LFP:   (Train Reward) {eval_stats_train_lfp['reward']:.2f}; (Train Acc) {eval_stats_train_lfp['accuracy']:.2f}; \"\n",
    "        f\"(Val Reward) {eval_stats_val_lfp['reward']:.2f}; (Val Acc) {eval_stats_val_lfp['accuracy']:.2f}\\n\"\n",
    "        f\"  Grad:   (Train Loss) {eval_stats_train_surr['loss']:.2f}; (Train Acc) {eval_stats_train_surr['accuracy']:.2f}; \"\n",
    "        f\"(Val Loss) {eval_stats_val_surr['loss']:.2f}; (Val Acc) {eval_stats_val_surr['accuracy']:.2f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as font_manager\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# Set font properties and plot style (copied from beans-vit-training)\n",
    "font_path = plt.matplotlib.get_data_path() + \"/fonts/ttf/cmr10.ttf\"\n",
    "cmfont = font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.serif\"] = cmfont.get_name()\n",
    "plt.rcParams[\"mathtext.fontset\"] = \"cm\"\n",
    "plt.rcParams[\"font.size\"] = 15\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "plt.rcParams[\"axes.formatter.use_mathtext\"] = True\n",
    "plt.rcParams['axes.linewidth'] = 1.5\n",
    "\n",
    "# Prepare data\n",
    "epochs_range = list(eval_stats_per_epoch.keys())\n",
    "acc_lfp = [eval_stats_per_epoch[e][\"eval_val_lfp\"][\"accuracy\"] for e in epochs_range]\n",
    "acc_surr = [eval_stats_per_epoch[e][\"eval_val_surr\"][\"accuracy\"] for e in epochs_range]\n",
    "runtime_lfp = [timing_stats[e][\"lfp_total\"] for e in epochs_range]\n",
    "runtime_surr = [timing_stats[e][\"surr_total\"] for e in epochs_range]\n",
    "\n",
    "# Colormap and labels\n",
    "colors = np.linspace(0, 1, 2)\n",
    "palette = cm.get_cmap(\"Set1\")(colors)\n",
    "pastel = 0.3\n",
    "palette = (1 - pastel) * palette + pastel * np.ones((2, 4))\n",
    "\n",
    "LABELS = {\n",
    "    \"lfp\": r\"LFP-$\\varepsilon$\",\n",
    "    \"surr\": r\"Surrogate Grad\",\n",
    "}\n",
    "\n",
    "# Plot accuracy over epochs\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "xaxis = np.arange(len(epochs_range))\n",
    "\n",
    "ax.plot(xaxis, acc_lfp, color=palette[0], label=LABELS[\"lfp\"], linewidth=3.5, alpha=1)\n",
    "ax.plot(xaxis, acc_surr, color=palette[1], label=LABELS[\"surr\"], linewidth=3.5, alpha=1)\n",
    "\n",
    "linelocs = [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "ax.hlines(\n",
    "    linelocs,\n",
    "    xmin=-1,\n",
    "    xmax=xaxis[-1],\n",
    "    color=(0.5, 0.5, 0.5, 1),\n",
    "    linewidth=1.5,\n",
    "    zorder=0,\n",
    ")\n",
    "\n",
    "ax.set_ylabel(\"Validation Accuracy [%]\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylim([0.0, 1.01])\n",
    "ax.set_yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_xlim([0.0, xaxis[-1]])\n",
    "ax.set_yticklabels([0, 20, 40, 60, 80, 100])\n",
    "ax.tick_params(length=6, width=2)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot runtime vs. accuracy (logscale x-axis)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6, 3))\n",
    "ax.plot(np.cumsum(runtime_lfp), acc_lfp, color=palette[0], label=LABELS[\"lfp\"], marker=\"o\", linewidth=3.5)\n",
    "ax.plot(np.cumsum(runtime_surr), acc_surr, color=palette[1], label=LABELS[\"surr\"], marker=\"o\", linewidth=3.5)\n",
    "ax.set_xscale(\"log\")\n",
    "ax.set_xlabel(\"Cumulative Runtime (s, log scale)\")\n",
    "ax.set_ylabel(\"Validation Accuracy [%]\")\n",
    "ax.set_ylim([0.0, 1.01])\n",
    "ax.set_yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "ax.set_yticklabels([0, 20, 40, 60, 80, 100])\n",
    "ax.tick_params(length=6, width=2)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfprop-KukTaqIE-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
