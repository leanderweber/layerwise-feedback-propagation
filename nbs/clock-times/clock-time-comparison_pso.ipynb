{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook provides a minimal example for using LFP to train a simple LeNet on MNIST.\n",
    "\n",
    "For more complex examples, refer to the experiment notebooks in ./nbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T15:56:21.726701Z",
     "iopub.status.busy": "2025-03-31T15:56:21.726290Z",
     "iopub.status.idle": "2025-03-31T15:56:25.763479Z",
     "shell.execute_reply": "2025-03-31T15:56:25.762711Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lweber/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as tnn\n",
    "import torcheval.metrics\n",
    "import torchvision.datasets as tvisiondata\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "\n",
    "from lfprop.model.models import ACTIVATION_MAP\n",
    "\n",
    "from lfprop.propagation import (\n",
    "    propagator_lxt as propagator,\n",
    ")  # LFP propagator.\n",
    "from lfprop.rewards import reward_functions as rewards  # Reward Functions\n",
    "from lfprop.rewards import rewards as loss_fns\n",
    "from torch_pso import ParticleSwarmOptimizer\n",
    "from fa import *\n",
    "from dladmm import dladmm \n",
    "from dladmm import input_data as dladmm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T15:56:25.767079Z",
     "iopub.status.busy": "2025-03-31T15:56:25.766780Z",
     "iopub.status.idle": "2025-03-31T15:56:25.775513Z",
     "shell.execute_reply": "2025-03-31T15:56:25.774773Z"
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"lenet\"\n",
    "method_name = \"pso\" # lfp-epsilon, vanilla-gradient, pso, fa, dladmm | TODO ldtp, ga\n",
    "seed = 0\n",
    "epochs = 50\n",
    "\n",
    "data_path = \"your_data_path_here\"  # Path to your dataset\n",
    "savepath = \"your_save_path_here\" # Path to save results\n",
    "os.makedirs(savepath, exist_ok=True)\n",
    "\n",
    "n_channels = 1\n",
    "n_outputs = 10\n",
    "batch_size = 1000\n",
    "\n",
    "general_params = {\n",
    "    \"n_channels\": n_channels,\n",
    "    \"n_outputs\": n_outputs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": epochs\n",
    "}\n",
    "\n",
    "def set_random_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.enabled = False\n",
    "\n",
    "set_random_seeds(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T15:56:25.778489Z",
     "iopub.status.busy": "2025-03-31T15:56:25.778207Z",
     "iopub.status.idle": "2025-03-31T15:56:25.841325Z",
     "shell.execute_reply": "2025-03-31T15:56:25.840692Z"
    }
   },
   "outputs": [],
   "source": [
    "transform = T.Compose([T.ToTensor(), T.Normalize((0.5,), (0.5,))])\n",
    "training_data = tvisiondata.MNIST(\n",
    "    root=data_path,\n",
    "    transform=transform,\n",
    "    download=True,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "testing_data = tvisiondata.MNIST(\n",
    "    root=data_path,\n",
    "    transform=transform,\n",
    "    download=True,\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "testing_loader = torch.utils.data.DataLoader(testing_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T15:56:25.844167Z",
     "iopub.status.busy": "2025-03-31T15:56:25.843924Z",
     "iopub.status.idle": "2025-03-31T15:56:25.856413Z",
     "shell.execute_reply": "2025-03-31T15:56:25.855852Z"
    }
   },
   "outputs": [],
   "source": [
    "class MLP(tnn.Module):\n",
    "    \"\"\"\n",
    "    Small MLP\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels, n_outputs, activation=tnn.ReLU):\n",
    "        super().__init__()\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = tnn.Sequential(\n",
    "            tnn.Linear(28*28, 120),\n",
    "            activation(),\n",
    "            tnn.Linear(120, 84),\n",
    "            activation(),\n",
    "            tnn.Linear(84, n_outputs),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forwards input through network\n",
    "        \"\"\"\n",
    "\n",
    "        # Forward through network\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        # Return output\n",
    "        return x\n",
    "    \n",
    "class FaMLP(tnn.Module):\n",
    "    \"\"\"\n",
    "    Small MLP supporting feedback alignment\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels, n_outputs, activation=tnn.ReLU):\n",
    "        super().__init__()\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = tnn.Sequential(\n",
    "            LinearFA(28*28, 120),\n",
    "            activation(),\n",
    "            LinearFA(120, 84),\n",
    "            activation(),\n",
    "            LinearFA(84, n_outputs),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forwards input through network\n",
    "        \"\"\"\n",
    "\n",
    "        # Forward through network\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        # Return output\n",
    "        return x\n",
    "\n",
    "class LeNet(tnn.Module):\n",
    "    \"\"\"\n",
    "    Small LeNet\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels, n_outputs, activation=tnn.ReLU):\n",
    "        super().__init__()\n",
    "\n",
    "        # Feature extractor\n",
    "        self.features = tnn.Sequential(\n",
    "            tnn.Conv2d(n_channels, 16, 5),\n",
    "            activation(),\n",
    "            tnn.MaxPool2d(2, 2),\n",
    "            tnn.Conv2d(16, 16, 5),\n",
    "            activation(),\n",
    "            tnn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = tnn.Sequential(\n",
    "            tnn.Linear(256 if n_channels == 1 else 400, 120),\n",
    "            activation(),\n",
    "            tnn.Dropout(),\n",
    "            tnn.Linear(120, 84),\n",
    "            activation(),\n",
    "            tnn.Dropout(),\n",
    "        )\n",
    "\n",
    "        self.last = tnn.Linear(84, n_outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forwards input through network\n",
    "        \"\"\"\n",
    "\n",
    "        # Forward through network\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        x = self.last(x)\n",
    "\n",
    "        # Return output\n",
    "        return x\n",
    "    \n",
    "class FaLeNet(tnn.Module):\n",
    "    \"\"\"\n",
    "    Small LeNet supporting feedback alignment\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels, n_outputs, activation=tnn.ReLU):\n",
    "        super().__init__()\n",
    "\n",
    "        # Feature extractor\n",
    "        self.features = tnn.Sequential(\n",
    "            Conv2dFA(n_channels, 16, 5),\n",
    "            activation(),\n",
    "            tnn.MaxPool2d(2, 2),\n",
    "            Conv2dFA(16, 16, 5),\n",
    "            activation(),\n",
    "            tnn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = tnn.Sequential(\n",
    "            LinearFA(256 if n_channels == 1 else 400, 120),\n",
    "            activation(),\n",
    "            tnn.Dropout(),\n",
    "            LinearFA(120, 84),\n",
    "            activation(),\n",
    "            tnn.Dropout(),\n",
    "        )\n",
    "\n",
    "        self.last = LinearFA(84, n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forwards input through network\n",
    "        \"\"\"\n",
    "\n",
    "        # Forward through network\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        x = self.last(x)\n",
    "\n",
    "        # Return output\n",
    "        return x\n",
    "\n",
    "def name_modules(module, name):\n",
    "    \"\"\"\n",
    "    Recursive function to name modules for debugging \n",
    "    \"\"\"\n",
    "    \n",
    "    for cname, child in module.named_children():\n",
    "        child.tmpname = cname if name == \"\" else f\"{name}.{cname}\"\n",
    "        name_modules(child, child.tmpname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Training Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T15:56:25.859891Z",
     "iopub.status.busy": "2025-03-31T15:56:25.859629Z",
     "iopub.status.idle": "2025-03-31T15:56:25.877508Z",
     "shell.execute_reply": "2025-03-31T15:56:25.877020Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval_model(model, loader, objective_func):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a single dataset\n",
    "    \"\"\"\n",
    "    eval_metrics = {\n",
    "        \"objective\": torcheval.metrics.Mean(device=device),\n",
    "        \"accuracy\": torcheval.metrics.MulticlassAccuracy(average=\"micro\", num_classes=10, k=1, device=device),\n",
    "    }\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate over Data Loader\n",
    "    for index, (inputs, labels) in enumerate(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get model predictions\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # Get rewards\n",
    "            objective = objective_func(outputs, labels)\n",
    "\n",
    "        for k, v in eval_metrics.items():\n",
    "            if k == \"objective\":\n",
    "                eval_metrics[k].update(objective)\n",
    "            else:\n",
    "                eval_metrics[k].update(outputs, labels)\n",
    "\n",
    "    return_dict = {m: metric.compute().detach().cpu().numpy() for m, metric in eval_metrics.items()}\n",
    "\n",
    "    # Return evaluation\n",
    "    return return_dict\n",
    "\n",
    "def lfp_step(model, optimizer, objective_func, propagation_composite, inputs, labels):\n",
    "    \"\"\"\n",
    "    Performs a single training step using LFP. This is quite similar to a standard gradient descent training loop.\n",
    "    \"\"\"\n",
    "    # Set Model to training mode\n",
    "    model.train()\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        # Zero Optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # This applies LFP Hooks/Functions\n",
    "        with propagation_composite.context(model) as modified:\n",
    "            inputs = inputs.detach().requires_grad_(True)\n",
    "            outputs = modified(inputs)\n",
    "\n",
    "            # Calculate reward\n",
    "            # Do like this to avoid tensors being kept in memory\n",
    "            reward = torch.from_numpy(objective_func(outputs, labels).detach().cpu().numpy()).to(device)\n",
    "\n",
    "            # Calculate LFP and write into .feedback attribute of parameters\n",
    "            torch.autograd.grad((outputs,), (inputs,), grad_outputs=(reward,), retain_graph=False)[0]\n",
    "\n",
    "            # Write LFP Testues into .grad attributes. Note the negative sign: LFP requires maximization instead of minimization like gradient descent\n",
    "            for name, param in model.named_parameters():\n",
    "                param.grad = -param.feedback\n",
    "\n",
    "            # Update Clipping. Training may become unstable otherwise, especially in small models with large learning rates.\n",
    "            # In larger models (e.g., VGG, ResNet), where smaller learning rates are generally utilized, not clipping updates may result in better performance.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0, 2.0)\n",
    "\n",
    "            # Optimization step\n",
    "            optimizer.step()\n",
    "\n",
    "    # Set Model back to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "def grad_step(model, optimizer, objective_func, inputs, labels):\n",
    "    \"\"\"\n",
    "    Performs a single training step using Gradient Descent\n",
    "    \"\"\"\n",
    "    # Set Model to training mode\n",
    "    model.train()\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        # Zero Optimizer\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        inputs = inputs.detach()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate reward\n",
    "        # Do like this to avoid tensors being kept in memory\n",
    "        loss = objective_func(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update Clipping. Training may become unstable otherwise, especially in small models with large learning rates.\n",
    "        # In larger models (e.g., VGG, ResNet), where smaller learning rates are generally utilized, not clipping updates may result in better performance.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0, 2.0)\n",
    "\n",
    "        # Optimization step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Set Model back to eval mode\n",
    "    model.eval()\n",
    "\n",
    "def pso_step(model, optimizer, objective_func, inputs, labels, max_steps):\n",
    "    \"\"\"\n",
    "    Performs a single training step using PSO\n",
    "    \"\"\"\n",
    "    # Set Model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # def schedule_inertial_weight():\n",
    "    #     if hasattr(optimizer, \"inertial_weight_min\") and hasattr(optimizer, \"inertial_weight_max\"):\n",
    "    #         inertial_weight = optimizer.inertial_weight_max - (optimizer.inertial_weight_max - optimizer.inertial_weight_min)/max_steps * optimizer.current_step\n",
    "    #         optimizer.inertial_weight = inertial_weight\n",
    "    #         for particle in optimizer.particles:\n",
    "    #             particle.inertial_weight = inertial_weight\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Zero Optimizer\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        inputs = inputs.detach()\n",
    "\n",
    "        def closure():\n",
    "            # Clear any grads from before the optimization step, since we will be changing the parameters\n",
    "            optimizer.zero_grad()  \n",
    "            return objective_func(model(inputs), labels) # VERY IMPORTANT that model forward is INSIDE closure\n",
    "\n",
    "        # Optimization step\n",
    "        optimizer.step(closure)\n",
    "        #schedule_inertial_weight()\n",
    "\n",
    "    # Set Model back to eval mode\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "def train(model, optimizer, objective_func, propagation_composite, **kwargs):\n",
    "    \n",
    "    evals = {\n",
    "        \"train_accuracy\": [],\n",
    "        \"train_objective\": [],\n",
    "        \"test_accuracy\": [],\n",
    "        \"test_objective\": [],\n",
    "        \"clock_time\": [],\n",
    "    }\n",
    "    \n",
    "    eval_stats_train = eval_model(model, training_loader, objective_func)\n",
    "    eval_stats_test = eval_model(model, testing_loader, objective_func)\n",
    "    print(\n",
    "        \"INIT: (Train Objective) {:.2f}; (Train Accuracy) {:.2f}; (Test Objective) {:.2f}; (Test Accuracy) {:.2f}\".format(\n",
    "            float(np.mean(eval_stats_train[\"objective\"])),\n",
    "            float(eval_stats_train[\"accuracy\"]),\n",
    "            float(np.mean(eval_stats_test[\"objective\"])),\n",
    "            float(eval_stats_test[\"accuracy\"]),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Iterate over Data Loader\n",
    "        pre = time.time()\n",
    "        for index, (inputs, labels) in enumerate(training_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = torch.tensor(labels).to(device)\n",
    "\n",
    "            # Perform Update Step\n",
    "            if propagation_composite is None:\n",
    "                grad_step(model, optimizer, objective_func, inputs, labels)\n",
    "            elif propagation_composite == \"pso\":\n",
    "                pso_step(model, optimizer, objective_func, inputs, labels, max_steps = len(training_loader)*epochs)\n",
    "            else:\n",
    "                lfp_step(model, optimizer, objective_func, propagation_composite, inputs, labels)\n",
    "            \n",
    "            # Log zero ratios\n",
    "            for cname, child in model.named_modules():\n",
    "                if hasattr(child, \"zeros_ratio\"):\n",
    "                    if f\"zeros_{child.tmpname}\" not in evals.keys():\n",
    "                        evals[f\"zeros_{child.tmpname}\"] = []\n",
    "                    evals[f\"zeros_{child.tmpname}\"].append(child.zeros_ratio)\n",
    "\n",
    "        post = time.time()\n",
    "\n",
    "        # Evaluate and print performance after every epoch\n",
    "        eval_stats_train = eval_model(model, training_loader, objective_func)\n",
    "        eval_stats_test = eval_model(model, testing_loader, objective_func)\n",
    "        print(\n",
    "            \"Epoch {}/{}: (Train Objective) {:.2f}; (Train Accuracy) {:.2f}; (Test Objective) {:.2f}; (Test Accuracy) {:.2f}\".format(\n",
    "                epoch + 1,\n",
    "                epochs,\n",
    "                float(np.mean(eval_stats_train[\"objective\"])),\n",
    "                float(eval_stats_train[\"accuracy\"]),\n",
    "                float(np.mean(eval_stats_test[\"objective\"])),\n",
    "                float(eval_stats_test[\"accuracy\"]),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        evals[\"train_accuracy\"].append(float(eval_stats_train[\"accuracy\"]))\n",
    "        evals[\"train_objective\"].append(float(eval_stats_train[\"objective\"]))\n",
    "        evals[\"test_accuracy\"].append(float(eval_stats_test[\"accuracy\"]))\n",
    "        evals[\"test_objective\"].append(float(eval_stats_test[\"objective\"]))\n",
    "        evals[\"clock_time\"].append(post-pre)\n",
    "        \n",
    "    return evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Training Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T15:56:25.880682Z",
     "iopub.status.busy": "2025-03-31T15:56:25.880424Z",
     "iopub.status.idle": "2025-03-31T15:56:27.759557Z",
     "shell.execute_reply": "2025-03-31T15:56:27.758824Z"
    }
   },
   "outputs": [],
   "source": [
    "if method_name == \"lfp-epsilon\":\n",
    "    model_class = MLP if model_name == \"mlp\" else LeNet\n",
    "    model = model_class(\n",
    "        n_channels=n_channels,\n",
    "        n_outputs=n_outputs,\n",
    "        activation=tnn.ReLU\n",
    "    )\n",
    "    name_modules(model, \"\")\n",
    "    model.tmpname = \"root\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    training_cfg = {\n",
    "        \"model_class\": model_class,\n",
    "        \"propagation_composite\": propagator.LFPEpsilonComposite(),\n",
    "        \"objective_func\": rewards.SoftmaxLossReward(device),\n",
    "        \"model\": model,\n",
    "        \"optimizer\": torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9),\n",
    "    }\n",
    "    training_func = train\n",
    "\n",
    "elif method_name == \"vanilla-gradient\":\n",
    "    model_class = MLP if model_name == \"mlp\" else LeNet\n",
    "    model = model_class(\n",
    "        n_channels=n_channels,\n",
    "        n_outputs=n_outputs,\n",
    "        activation=tnn.ReLU\n",
    "    )\n",
    "    name_modules(model, \"\")\n",
    "    model.tmpname = \"root\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    training_cfg = {\n",
    "        \"model_class\": model_class,\n",
    "        \"propagation_composite\": None,\n",
    "        \"objective_func\": loss_fns.CustomCrossEntropyLoss(),\n",
    "        \"model\": model,\n",
    "        \"optimizer\": torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9),\n",
    "    }\n",
    "    training_func = train\n",
    "\n",
    "elif method_name == \"pso\":\n",
    "    model_class = MLP if model_name == \"mlp\" else LeNet\n",
    "    model = model_class(\n",
    "            n_channels=n_channels,\n",
    "            n_outputs=n_outputs,\n",
    "            activation=tnn.ReLU\n",
    "        ) \n",
    "    name_modules(model, \"\")\n",
    "    model.tmpname = \"root\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    training_cfg = {\n",
    "        \"model_class\": model_class,\n",
    "        \"propagation_composite\": \"pso\",\n",
    "        \"objective_func\": torch.nn.CrossEntropyLoss(),\n",
    "        #\"objective_func\": loss_fns.CustomCrossEntropyLoss(),\n",
    "        \"model\": model,\n",
    "        \"optimizer\": ParticleSwarmOptimizer(\n",
    "            model.parameters(), # TODO: tune hyperparams\n",
    "            cognitive_coefficient=2, # Note: We decay this if particle best was not update for a while\n",
    "            social_coefficient=2, # Note: We decay this if global best was not update for a while\n",
    "            inertial_weight=0.8,\n",
    "            num_particles=1000,\n",
    "            max_param_value=0.1,\n",
    "            min_param_value=-0.1\n",
    "        )   \n",
    "    }\n",
    "    training_func = train\n",
    "\n",
    "elif method_name == \"fa\":\n",
    "    model_class = FaMLP if model_name == \"mlp\" else FaLeNet\n",
    "    model = model_class(\n",
    "        n_channels=n_channels,\n",
    "        n_outputs=n_outputs,\n",
    "        activation=tnn.ReLU\n",
    "    )\n",
    "    name_modules(model, \"\")\n",
    "    model.tmpname = \"root\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    training_cfg = {\n",
    "        \"model_class\": model_class,\n",
    "        \"propagation_composite\": None,\n",
    "        \"objective_func\": loss_fns.CustomCrossEntropyLoss(),\n",
    "        \"model\": model,\n",
    "        \"optimizer\": torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9),\n",
    "    }\n",
    "    training_func = train\n",
    "\n",
    "# elif method_name == \"ldtp\": #https://arxiv.org/pdf/2201.13415\n",
    "    \n",
    "elif method_name == \"dladmm\":\n",
    "    if model_name != \"mlp\":\n",
    "        raise ValueError(\"dladmm only implemented for mlp\")\n",
    "    dladmm_mnist = dladmm_data.mnist(data_path)\n",
    "    model_class = dladmm.DladmmNet\n",
    "    model = tuple(model_class(\n",
    "        images=torch.transpose(dladmm_mnist.x_train, 0, 1), \n",
    "        label=torch.transpose(dladmm_mnist.y_train, 0, 1), \n",
    "        num_of_neurons1=120, \n",
    "        num_of_neurons2=84,\n",
    "    )) # Model is just a tuple of parameters, pre-acts, and activations here\n",
    "\n",
    "    training_cfg = {\n",
    "        \"model_class\": model_class,\n",
    "        \"model\": model,\n",
    "        \"x_train\": dladmm_mnist.x_train,\n",
    "        \"y_train\": dladmm_mnist.y_train,\n",
    "        \"x_test\": dladmm_mnist.x_test,\n",
    "        \"y_test\": dladmm_mnist.y_test,\n",
    "    }\n",
    "    training_func = dladmm.train\n",
    "\n",
    "else:\n",
    "    raise ValueError\n",
    "\n",
    "training_cfg = {**training_cfg, **general_params}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, if results not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-31T15:56:27.762995Z",
     "iopub.status.busy": "2025-03-31T15:56:27.762726Z",
     "iopub.status.idle": "2025-04-01T23:18:35.122115Z",
     "shell.execute_reply": "2025-04-01T23:18:35.121445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING method pso with seed 13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT: (Train Objective) 2.31; (Train Accuracy) 0.10; (Test Objective) 2.31; (Test Accuracy) 0.10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: 7.754240036010742 better than inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: 2.5245656967163086 better than 7.754240036010742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 35: 2.514148712158203 better than 2.5245656967163086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 36: 2.512770175933838 better than 2.514148712158203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 57: 2.5069611072540283 better than 2.512770175933838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 58: 2.497843027114868 better than 2.5069611072540283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: (Train Objective) 2.28; (Train Accuracy) 0.14; (Test Objective) 2.28; (Test Accuracy) 0.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 69: 2.4436609745025635 better than 2.497843027114868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 88: 2.4348886013031006 better than 2.4436609745025635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 90: 2.4331562519073486 better than 2.4348886013031006\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 106: 2.417565107345581 better than 2.4331562519073486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 106: 2.4129135608673096 better than 2.417565107345581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 116: 2.4078598022460938 better than 2.4129135608673096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: (Train Objective) 2.28; (Train Accuracy) 0.14; (Test Objective) 2.28; (Test Accuracy) 0.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 125: 2.3760595321655273 better than 2.4078598022460938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 140: 2.375542402267456 better than 2.3760595321655273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 140: 2.3689382076263428 better than 2.375542402267456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 160: 2.3667213916778564 better than 2.3689382076263428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 160: 2.357557535171509 better than 2.3667213916778564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 160: 2.3527743816375732 better than 2.357557535171509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 169: 2.3476033210754395 better than 2.3527743816375732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: (Train Objective) 2.27; (Train Accuracy) 0.12; (Test Objective) 2.27; (Test Accuracy) 0.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 183: 2.345689058303833 better than 2.3476033210754395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 201: 2.34421443939209 better than 2.345689058303833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 201: 2.3419158458709717 better than 2.34421443939209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 213: 2.306917905807495 better than 2.3419158458709717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: (Train Objective) 2.26; (Train Accuracy) 0.15; (Test Objective) 2.25; (Test Accuracy) 0.15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 251: 2.30627179145813 better than 2.306917905807495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 253: 2.305394172668457 better than 2.30627179145813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 270: 2.289822578430176 better than 2.305394172668457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 277: 2.28743577003479 better than 2.289822578430176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 277: 2.2801878452301025 better than 2.28743577003479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: (Train Objective) 2.23; (Train Accuracy) 0.19; (Test Objective) 2.22; (Test Accuracy) 0.19\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 305: 2.2643439769744873 better than 2.2801878452301025\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 317: 2.264163017272949 better than 2.2643439769744873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 338: 2.260317325592041 better than 2.264163017272949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: (Train Objective) 2.22; (Train Accuracy) 0.20; (Test Objective) 2.22; (Test Accuracy) 0.20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 410: 2.25964093208313 better than 2.260317325592041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: (Train Objective) 2.21; (Train Accuracy) 0.21; (Test Objective) 2.21; (Test Accuracy) 0.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 437: 2.2585349082946777 better than 2.25964093208313\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: (Train Objective) 2.21; (Train Accuracy) 0.21; (Test Objective) 2.21; (Test Accuracy) 0.21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 502: 2.2555508613586426 better than 2.2585349082946777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 510: 2.254072427749634 better than 2.2555508613586426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 604: 2.253028392791748 better than 2.254072427749634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1033: 2.252627372741699 better than 2.253028392791748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1359: 2.248192071914673 better than 2.252627372741699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2001: 2.247478723526001 better than 2.248192071914673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2327: 2.2388834953308105 better than 2.247478723526001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: (Train Objective) 2.21; (Train Accuracy) 0.22; (Test Objective) 2.20; (Test Accuracy) 0.22\n",
      "FINAL EPOCH\n",
      "train_accuracy 0.219200000166893\n",
      "FINAL EPOCH\n",
      "train_objective 2.207744765281677\n",
      "FINAL EPOCH\n",
      "test_accuracy 0.2249000072479248\n",
      "FINAL EPOCH\n",
      "test_objective 2.2046017169952394\n",
      "FINAL EPOCH\n",
      "clock_time 2099.4157853126526\n"
     ]
    }
   ],
   "source": [
    "result_path = os.path.join(savepath, \"result_dict.joblib\")\n",
    "if not os.path.exists(result_path):\n",
    "    print(f\"TRAINING method {method_name} with seed {seed}\")\n",
    "    evals = training_func(**training_cfg)\n",
    "    joblib.dump(evals, result_path)\n",
    "else:\n",
    "    evals = joblib.load(result_path)\n",
    "\n",
    "for k, v in evals.items():\n",
    "    print(\"FINAL EPOCH\")\n",
    "    print(k, v[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfprop-KukTaqIE-py3.11 (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
