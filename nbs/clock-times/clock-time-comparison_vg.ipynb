{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook provides a minimal example for using LFP to train a simple LeNet on MNIST.\n",
    "\n",
    "For more complex examples, refer to the experiment notebooks in ./nbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lweber/.cache/pypoetry/virtualenvs/lfprop-KukTaqIE-py3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as tnn\n",
    "import torcheval.metrics\n",
    "import torchvision.datasets as tvisiondata\n",
    "import torchvision.transforms as T\n",
    "from tqdm import tqdm\n",
    "\n",
    "from experiment_utils.model.models import ACTIVATION_MAP\n",
    "\n",
    "from lfprop.propagation import (\n",
    "    propagator_lxt as propagator,\n",
    ")  # LFP propagator. Alternatively, use propagator_zennit\n",
    "from lfprop.rewards import reward_functions as rewards  # Reward Functions\n",
    "from lfprop.rewards import rewards as loss_fns\n",
    "from torch_pso import ParticleSwarmOptimizer\n",
    "from fa import *\n",
    "from dladmm import dladmm \n",
    "from dladmm import input_data as dladmm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"lenet\"\n",
    "method_name = \"vanilla-gradient\" # lfp-epsilon, vanilla-gradient, pso, fa, dladmm | TODO ldtp, ga\n",
    "seed = 0\n",
    "epochs = 50\n",
    "\n",
    "data_path = \"/media/lweber/f3ed2aae-a7bf-4a55-b50d-ea8fb534f1f52/Datasets/mnist\"\n",
    "savepath = f\"/media/lweber/f3ed2aae-a7bf-4a55-b50d-ea8fb534f1f52/reward-backprop/resubmission-1-experiments/clocktime-comparison/{method_name}-{model_name}-{seed}-{epochs}\"\n",
    "os.makedirs(savepath, exist_ok=True)\n",
    "\n",
    "n_channels = 1\n",
    "n_outputs = 10\n",
    "batch_size = 128\n",
    "\n",
    "general_params = {\n",
    "    \"n_channels\": n_channels,\n",
    "    \"n_outputs\": n_outputs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"epochs\": epochs\n",
    "}\n",
    "\n",
    "def set_random_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.enabled = False\n",
    "\n",
    "set_random_seeds(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.Compose([T.ToTensor(), T.Normalize((0.5,), (0.5,))])\n",
    "training_data = tvisiondata.MNIST(\n",
    "    root=data_path,\n",
    "    transform=transform,\n",
    "    download=True,\n",
    "    train=True,\n",
    ")\n",
    "\n",
    "testing_data = tvisiondata.MNIST(\n",
    "    root=data_path,\n",
    "    transform=transform,\n",
    "    download=True,\n",
    "    train=False,\n",
    ")\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "testing_loader = torch.utils.data.DataLoader(testing_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tnn.Module):\n",
    "    \"\"\"\n",
    "    Small MLP\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels, n_outputs, activation=tnn.ReLU):\n",
    "        super().__init__()\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = tnn.Sequential(\n",
    "            tnn.Linear(28*28, 120),\n",
    "            activation(),\n",
    "            tnn.Linear(120, 84),\n",
    "            activation(),\n",
    "            tnn.Linear(84, n_outputs),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forwards input through network\n",
    "        \"\"\"\n",
    "\n",
    "        # Forward through network\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        # Return output\n",
    "        return x\n",
    "    \n",
    "class FaMLP(tnn.Module):\n",
    "    \"\"\"\n",
    "    Small MLP supporting feedback alignment\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels, n_outputs, activation=tnn.ReLU):\n",
    "        super().__init__()\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = tnn.Sequential(\n",
    "            LinearFA(28*28, 120),\n",
    "            activation(),\n",
    "            LinearFA(120, 84),\n",
    "            activation(),\n",
    "            LinearFA(84, n_outputs),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forwards input through network\n",
    "        \"\"\"\n",
    "\n",
    "        # Forward through network\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        # Return output\n",
    "        return x\n",
    "\n",
    "class LeNet(tnn.Module):\n",
    "    \"\"\"\n",
    "    Small LeNet\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels, n_outputs, activation=tnn.ReLU):\n",
    "        super().__init__()\n",
    "\n",
    "        # Feature extractor\n",
    "        self.features = tnn.Sequential(\n",
    "            tnn.Conv2d(n_channels, 16, 5),\n",
    "            activation(),\n",
    "            tnn.MaxPool2d(2, 2),\n",
    "            tnn.Conv2d(16, 16, 5),\n",
    "            activation(),\n",
    "            tnn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = tnn.Sequential(\n",
    "            tnn.Linear(256 if n_channels == 1 else 400, 120),\n",
    "            activation(),\n",
    "            tnn.Dropout(),\n",
    "            tnn.Linear(120, 84),\n",
    "            activation(),\n",
    "            tnn.Dropout(),\n",
    "        )\n",
    "\n",
    "        self.last = tnn.Linear(84, n_outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forwards input through network\n",
    "        \"\"\"\n",
    "\n",
    "        # Forward through network\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        x = self.last(x)\n",
    "\n",
    "        # Return output\n",
    "        return x\n",
    "    \n",
    "class FaLeNet(tnn.Module):\n",
    "    \"\"\"\n",
    "    Small LeNet supporting feedback alignment\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels, n_outputs, activation=tnn.ReLU):\n",
    "        super().__init__()\n",
    "\n",
    "        # Feature extractor\n",
    "        self.features = tnn.Sequential(\n",
    "            Conv2dFA(n_channels, 16, 5),\n",
    "            activation(),\n",
    "            tnn.MaxPool2d(2, 2),\n",
    "            Conv2dFA(16, 16, 5),\n",
    "            activation(),\n",
    "            tnn.MaxPool2d(2, 2),\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = tnn.Sequential(\n",
    "            LinearFA(256 if n_channels == 1 else 400, 120),\n",
    "            activation(),\n",
    "            tnn.Dropout(),\n",
    "            LinearFA(120, 84),\n",
    "            activation(),\n",
    "            tnn.Dropout(),\n",
    "        )\n",
    "\n",
    "        self.last = LinearFA(84, n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forwards input through network\n",
    "        \"\"\"\n",
    "\n",
    "        # Forward through network\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        x = self.last(x)\n",
    "\n",
    "        # Return output\n",
    "        return x\n",
    "\n",
    "def name_modules(module, name):\n",
    "    \"\"\"\n",
    "    Recursive function to name modules for debugging \n",
    "    \"\"\"\n",
    "    \n",
    "    for cname, child in module.named_children():\n",
    "        child.tmpname = cname if name == \"\" else f\"{name}.{cname}\"\n",
    "        name_modules(child, child.tmpname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation and Training Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, loader, objective_func):\n",
    "    \"\"\"\n",
    "    Evaluates the model on a single dataset\n",
    "    \"\"\"\n",
    "    eval_metrics = {\n",
    "        \"objective\": torcheval.metrics.Mean(device=device),\n",
    "        \"accuracy\": torcheval.metrics.MulticlassAccuracy(average=\"micro\", num_classes=10, k=1, device=device),\n",
    "    }\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Iterate over Data Loader\n",
    "    for index, (inputs, labels) in enumerate(loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = torch.tensor(labels).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get model predictions\n",
    "            outputs = model(inputs)\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # Get rewards\n",
    "            objective = objective_func(outputs, labels)\n",
    "\n",
    "        for k, v in eval_metrics.items():\n",
    "            if k == \"objective\":\n",
    "                eval_metrics[k].update(objective)\n",
    "            else:\n",
    "                eval_metrics[k].update(outputs, labels)\n",
    "\n",
    "    return_dict = {m: metric.compute().detach().cpu().numpy() for m, metric in eval_metrics.items()}\n",
    "\n",
    "    # Return evaluation\n",
    "    return return_dict\n",
    "\n",
    "def lfp_step(model, optimizer, objective_func, propagation_composite, inputs, labels):\n",
    "    \"\"\"\n",
    "    Performs a single training step using LFP. This is quite similar to a standard gradient descent training loop.\n",
    "    \"\"\"\n",
    "    # Set Model to training mode\n",
    "    model.train()\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        # Zero Optimizer\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # This applies LFP Hooks/Functions (which depends on whether lxt or zennit backend is used)\n",
    "        with propagation_composite.context(model) as modified:\n",
    "            inputs = inputs.detach().requires_grad_(True)\n",
    "            outputs = modified(inputs)\n",
    "\n",
    "            # Calculate reward\n",
    "            # Do like this to avoid tensors being kept in memory\n",
    "            reward = torch.from_numpy(objective_func(outputs, labels).detach().cpu().numpy()).to(device)\n",
    "\n",
    "            # Calculate LFP and write into .feedback attribute of parameters\n",
    "            torch.autograd.grad((outputs,), (inputs,), grad_outputs=(reward,), retain_graph=False)[0]\n",
    "\n",
    "            # Write LFP Testues into .grad attributes. Note the negative sign: LFP requires maximization instead of minimization like gradient descent\n",
    "            for name, param in model.named_parameters():\n",
    "                param.grad = -param.feedback\n",
    "\n",
    "            # Update Clipping. Training may become unstable otherwise, especially in small models with large learning rates.\n",
    "            # In larger models (e.g., VGG, ResNet), where smaller learning rates are generally utilized, not clipping updates may result in better performance.\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0, 2.0)\n",
    "\n",
    "            # Optimization step\n",
    "            optimizer.step()\n",
    "\n",
    "    # Set Model back to eval mode\n",
    "    model.eval()\n",
    "    \n",
    "def grad_step(model, optimizer, objective_func, inputs, labels):\n",
    "    \"\"\"\n",
    "    Performs a single training step using Gradient Descent\n",
    "    \"\"\"\n",
    "    # Set Model to training mode\n",
    "    model.train()\n",
    "\n",
    "    with torch.enable_grad():\n",
    "        # Zero Optimizer\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        inputs = inputs.detach()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate reward\n",
    "        # Do like this to avoid tensors being kept in memory\n",
    "        loss = objective_func(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update Clipping. Training may become unstable otherwise, especially in small models with large learning rates.\n",
    "        # In larger models (e.g., VGG, ResNet), where smaller learning rates are generally utilized, not clipping updates may result in better performance.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 3.0, 2.0)\n",
    "\n",
    "        # Optimization step\n",
    "        optimizer.step()\n",
    "\n",
    "    # Set Model back to eval mode\n",
    "    model.eval()\n",
    "\n",
    "def pso_step(model, optimizer, objective_func, inputs, labels, max_steps):\n",
    "    \"\"\"\n",
    "    Performs a single training step using PSO\n",
    "    \"\"\"\n",
    "    # Set Model to training mode\n",
    "    model.train()\n",
    "\n",
    "    # def schedule_inertial_weight():\n",
    "    #     if hasattr(optimizer, \"inertial_weight_min\") and hasattr(optimizer, \"inertial_weight_max\"):\n",
    "    #         inertial_weight = optimizer.inertial_weight_max - (optimizer.inertial_weight_max - optimizer.inertial_weight_min)/max_steps * optimizer.current_step\n",
    "    #         optimizer.inertial_weight = inertial_weight\n",
    "    #         for particle in optimizer.particles:\n",
    "    #             particle.inertial_weight = inertial_weight\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Zero Optimizer\n",
    "        optimizer.zero_grad()\n",
    "            \n",
    "        inputs = inputs.detach()\n",
    "\n",
    "        def closure():\n",
    "            # Clear any grads from before the optimization step, since we will be changing the parameters\n",
    "            optimizer.zero_grad()  \n",
    "            return objective_func(model(inputs), labels) # VERY IMPORTANT that model forward is INSIDE closure\n",
    "\n",
    "        # Optimization step\n",
    "        optimizer.step(closure)\n",
    "        #schedule_inertial_weight()\n",
    "\n",
    "    # Set Model back to eval mode\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "def train(model, optimizer, objective_func, propagation_composite, **kwargs):\n",
    "    \n",
    "    evals = {\n",
    "        \"train_accuracy\": [],\n",
    "        \"train_objective\": [],\n",
    "        \"test_accuracy\": [],\n",
    "        \"test_objective\": [],\n",
    "        \"clock_time\": [],\n",
    "    }\n",
    "    \n",
    "    eval_stats_train = eval_model(model, training_loader, objective_func)\n",
    "    eval_stats_test = eval_model(model, testing_loader, objective_func)\n",
    "    print(\n",
    "        \"INIT: (Train Objective) {:.2f}; (Train Accuracy) {:.2f}; (Test Objective) {:.2f}; (Test Accuracy) {:.2f}\".format(\n",
    "            float(np.mean(eval_stats_train[\"objective\"])),\n",
    "            float(eval_stats_train[\"accuracy\"]),\n",
    "            float(np.mean(eval_stats_test[\"objective\"])),\n",
    "            float(eval_stats_test[\"accuracy\"]),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Iterate over Data Loader\n",
    "        pre = time.time()\n",
    "        for index, (inputs, labels) in enumerate(training_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = torch.tensor(labels).to(device)\n",
    "\n",
    "            # Perform Update Step\n",
    "            if propagation_composite is None:\n",
    "                grad_step(model, optimizer, objective_func, inputs, labels)\n",
    "            elif propagation_composite == \"pso\":\n",
    "                pso_step(model, optimizer, objective_func, inputs, labels, max_steps = len(training_loader)*epochs)\n",
    "            else:\n",
    "                lfp_step(model, optimizer, objective_func, propagation_composite, inputs, labels)\n",
    "            \n",
    "            # Log zero ratios\n",
    "            for cname, child in model.named_modules():\n",
    "                if hasattr(child, \"zeros_ratio\"):\n",
    "                    if f\"zeros_{child.tmpname}\" not in evals.keys():\n",
    "                        evals[f\"zeros_{child.tmpname}\"] = []\n",
    "                    evals[f\"zeros_{child.tmpname}\"].append(child.zeros_ratio)\n",
    "\n",
    "        post = time.time()\n",
    "\n",
    "        # Evaluate and print performance after every epoch\n",
    "        eval_stats_train = eval_model(model, training_loader, objective_func)\n",
    "        eval_stats_test = eval_model(model, testing_loader, objective_func)\n",
    "        print(\n",
    "            \"Epoch {}/{}: (Train Objective) {:.2f}; (Train Accuracy) {:.2f}; (Test Objective) {:.2f}; (Test Accuracy) {:.2f}\".format(\n",
    "                epoch + 1,\n",
    "                epochs,\n",
    "                float(np.mean(eval_stats_train[\"objective\"])),\n",
    "                float(eval_stats_train[\"accuracy\"]),\n",
    "                float(np.mean(eval_stats_test[\"objective\"])),\n",
    "                float(eval_stats_test[\"accuracy\"]),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        evals[\"train_accuracy\"].append(float(eval_stats_train[\"accuracy\"]))\n",
    "        evals[\"train_objective\"].append(float(eval_stats_train[\"objective\"]))\n",
    "        evals[\"test_accuracy\"].append(float(eval_stats_test[\"accuracy\"]))\n",
    "        evals[\"test_objective\"].append(float(eval_stats_test[\"objective\"]))\n",
    "        evals[\"clock_time\"].append(post-pre)\n",
    "        \n",
    "    return evals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up Training Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_class': <class '__main__.LeNet'>, 'propagation_composite': None, 'objective_func': CustomCrossEntropyLoss(), 'model': LeNet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=120, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (last): Linear(in_features=84, out_features=10, bias=True)\n",
      "), 'optimizer': SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")}\n"
     ]
    }
   ],
   "source": [
    "if method_name == \"lfp-epsilon\":\n",
    "    model_class = MLP if model_name == \"mlp\" else LeNet\n",
    "    model = model_class(\n",
    "        n_channels=n_channels,\n",
    "        n_outputs=n_outputs,\n",
    "        activation=tnn.ReLU\n",
    "    )\n",
    "    name_modules(model, \"\")\n",
    "    model.tmpname = \"root\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    training_cfg = {\n",
    "        \"model_class\": model_class,\n",
    "        \"propagation_composite\": propagator.LFPEpsilonComposite(),\n",
    "        \"objective_func\": rewards.SoftmaxLossReward(device),\n",
    "        \"model\": model,\n",
    "        \"optimizer\": torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9),\n",
    "    }\n",
    "    training_func = train\n",
    "\n",
    "elif method_name == \"vanilla-gradient\":\n",
    "    model_class = MLP if model_name == \"mlp\" else LeNet\n",
    "    model = model_class(\n",
    "        n_channels=n_channels,\n",
    "        n_outputs=n_outputs,\n",
    "        activation=tnn.ReLU\n",
    "    )\n",
    "    name_modules(model, \"\")\n",
    "    model.tmpname = \"root\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    training_cfg = {\n",
    "        \"model_class\": model_class,\n",
    "        \"propagation_composite\": None,\n",
    "        \"objective_func\": loss_fns.CustomCrossEntropyLoss(),\n",
    "        \"model\": model,\n",
    "        \"optimizer\": torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9),\n",
    "    }\n",
    "    training_func = train\n",
    "\n",
    "    print(training_cfg)\n",
    "\n",
    "elif method_name == \"pso\":\n",
    "    model_class = MLP if model_name == \"mlp\" else LeNet\n",
    "    model = model_class(\n",
    "            n_channels=n_channels,\n",
    "            n_outputs=n_outputs,\n",
    "            activation=tnn.ReLU\n",
    "        ) \n",
    "    name_modules(model, \"\")\n",
    "    model.tmpname = \"root\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    training_cfg = {\n",
    "        \"model_class\": model_class,\n",
    "        \"propagation_composite\": \"pso\",\n",
    "        \"objective_func\": torch.nn.CrossEntropyLoss(),\n",
    "        #\"objective_func\": loss_fns.CustomCrossEntropyLoss(),\n",
    "        \"model\": model,\n",
    "        \"optimizer\": ParticleSwarmOptimizer(\n",
    "            model.parameters(), # TODO: tune hyperparams\n",
    "            cognitive_coefficient=2, # Note: We decay this if particle best was not update for a while\n",
    "            social_coefficient=2, # Note: We decay this if global best was not update for a while\n",
    "            inertial_weight=0.8,\n",
    "            num_particles=1000,\n",
    "            max_param_value=0.1,\n",
    "            min_param_value=-0.1\n",
    "        )   \n",
    "    }\n",
    "    training_func = train\n",
    "\n",
    "elif method_name == \"fa\":\n",
    "    model_class = FaMLP if model_name == \"mlp\" else FaLeNet\n",
    "    model = model_class(\n",
    "        n_channels=n_channels,\n",
    "        n_outputs=n_outputs,\n",
    "        activation=tnn.ReLU\n",
    "    )\n",
    "    name_modules(model, \"\")\n",
    "    model.tmpname = \"root\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    training_cfg = {\n",
    "        \"model_class\": model_class,\n",
    "        \"propagation_composite\": None,\n",
    "        \"objective_func\": loss_fns.CustomCrossEntropyLoss(),\n",
    "        \"model\": model,\n",
    "        \"optimizer\": torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9),\n",
    "    }\n",
    "    training_func = train\n",
    "\n",
    "# elif method_name == \"ldtp\": #https://arxiv.org/pdf/2201.13415\n",
    "    \n",
    "elif method_name == \"dladmm\":\n",
    "    if model_name != \"mlp\":\n",
    "        raise ValueError(\"dladmm only implemented for mlp\")\n",
    "    dladmm_mnist = dladmm_data.mnist(data_path)\n",
    "    model_class = dladmm.DladmmNet\n",
    "    model = tuple(model_class(\n",
    "        images=torch.transpose(dladmm_mnist.x_train, 0, 1), \n",
    "        label=torch.transpose(dladmm_mnist.y_train, 0, 1), \n",
    "        num_of_neurons1=120, \n",
    "        num_of_neurons2=84,\n",
    "    )) # Model is just a tuple of parameters, pre-acts, and activations here\n",
    "\n",
    "    training_cfg = {\n",
    "        \"model_class\": model_class,\n",
    "        \"model\": model,\n",
    "        \"x_train\": dladmm_mnist.x_train,\n",
    "        \"y_train\": dladmm_mnist.y_train,\n",
    "        \"x_test\": dladmm_mnist.x_test,\n",
    "        \"y_test\": dladmm_mnist.y_test,\n",
    "    }\n",
    "    training_func = dladmm.train\n",
    "\n",
    "else:\n",
    "    raise ValueError\n",
    "\n",
    "training_cfg = {**training_cfg, **general_params}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, if results not available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING method vanilla-gradient with seed 13\n",
      "INIT: (Train Objective) 2.31; (Train Accuracy) 0.10; (Test Objective) 2.31; (Test Accuracy) 0.10\n",
      "Epoch 1/50: (Train Objective) 0.14; (Train Accuracy) 0.96; (Test Objective) 0.12; (Test Accuracy) 0.96\n",
      "Epoch 2/50: (Train Objective) 0.08; (Train Accuracy) 0.98; (Test Objective) 0.07; (Test Accuracy) 0.98\n",
      "Epoch 3/50: (Train Objective) 0.06; (Train Accuracy) 0.98; (Test Objective) 0.06; (Test Accuracy) 0.98\n",
      "Epoch 4/50: (Train Objective) 0.05; (Train Accuracy) 0.98; (Test Objective) 0.05; (Test Accuracy) 0.98\n",
      "Epoch 5/50: (Train Objective) 0.04; (Train Accuracy) 0.99; (Test Objective) 0.04; (Test Accuracy) 0.99\n",
      "Epoch 6/50: (Train Objective) 0.04; (Train Accuracy) 0.99; (Test Objective) 0.04; (Test Accuracy) 0.99\n",
      "Epoch 7/50: (Train Objective) 0.03; (Train Accuracy) 0.99; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 8/50: (Train Objective) 0.03; (Train Accuracy) 0.99; (Test Objective) 0.04; (Test Accuracy) 0.99\n",
      "Epoch 9/50: (Train Objective) 0.03; (Train Accuracy) 0.99; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 10/50: (Train Objective) 0.03; (Train Accuracy) 0.99; (Test Objective) 0.04; (Test Accuracy) 0.99\n",
      "Epoch 11/50: (Train Objective) 0.02; (Train Accuracy) 0.99; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 12/50: (Train Objective) 0.02; (Train Accuracy) 0.99; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 13/50: (Train Objective) 0.02; (Train Accuracy) 0.99; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 14/50: (Train Objective) 0.02; (Train Accuracy) 0.99; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 15/50: (Train Objective) 0.02; (Train Accuracy) 0.99; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 16/50: (Train Objective) 0.02; (Train Accuracy) 0.99; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 17/50: (Train Objective) 0.02; (Train Accuracy) 0.99; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 18/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 19/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 20/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 21/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 22/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 23/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 24/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 25/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 26/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 27/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 28/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 29/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 30/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 31/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 32/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 33/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 34/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 35/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 36/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 37/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 38/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 39/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 40/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 41/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 42/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 43/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 44/50: (Train Objective) 0.01; (Train Accuracy) 1.00; (Test Objective) 0.04; (Test Accuracy) 0.99\n",
      "Epoch 45/50: (Train Objective) 0.00; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 46/50: (Train Objective) 0.00; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 47/50: (Train Objective) 0.00; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "Epoch 48/50: (Train Objective) 0.00; (Train Accuracy) 1.00; (Test Objective) 0.04; (Test Accuracy) 0.99\n",
      "Epoch 49/50: (Train Objective) 0.00; (Train Accuracy) 1.00; (Test Objective) 0.04; (Test Accuracy) 0.99\n",
      "Epoch 50/50: (Train Objective) 0.00; (Train Accuracy) 1.00; (Test Objective) 0.03; (Test Accuracy) 0.99\n",
      "FINAL EPOCH\n",
      "train_accuracy 0.9989666938781738\n",
      "FINAL EPOCH\n",
      "train_objective 0.0037512390542844497\n",
      "FINAL EPOCH\n",
      "test_accuracy 0.9919999837875366\n",
      "FINAL EPOCH\n",
      "test_objective 0.03425384691492262\n",
      "FINAL EPOCH\n",
      "clock_time 22.710516929626465\n"
     ]
    }
   ],
   "source": [
    "result_path = os.path.join(savepath, \"result_dict.joblib\")\n",
    "if not os.path.exists(result_path):\n",
    "    print(f\"TRAINING method {method_name} with seed {seed}\")\n",
    "    evals = training_func(**training_cfg)\n",
    "    joblib.dump(evals, result_path)\n",
    "else:\n",
    "    evals = joblib.load(result_path)\n",
    "\n",
    "for k, v in evals.items():\n",
    "    print(\"FINAL EPOCH\")\n",
    "    print(k, v[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lfprop-KukTaqIE-py3.11 (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
